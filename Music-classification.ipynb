{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496f2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255599cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blues  classical  country  disco  hiphop  jazz\tmetal  pop  reggae  rock\r\n"
     ]
    }
   ],
   "source": [
    "!ls music_dataset/genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fccaa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blues.00000.wav  blues.00025.wav  blues.00050.wav  blues.00075.wav\r\n",
      "blues.00001.wav  blues.00026.wav  blues.00051.wav  blues.00076.wav\r\n",
      "blues.00002.wav  blues.00027.wav  blues.00052.wav  blues.00077.wav\r\n",
      "blues.00003.wav  blues.00028.wav  blues.00053.wav  blues.00078.wav\r\n",
      "blues.00004.wav  blues.00029.wav  blues.00054.wav  blues.00079.wav\r\n",
      "blues.00005.wav  blues.00030.wav  blues.00055.wav  blues.00080.wav\r\n",
      "blues.00006.wav  blues.00031.wav  blues.00056.wav  blues.00081.wav\r\n",
      "blues.00007.wav  blues.00032.wav  blues.00057.wav  blues.00082.wav\r\n",
      "blues.00008.wav  blues.00033.wav  blues.00058.wav  blues.00083.wav\r\n",
      "blues.00009.wav  blues.00034.wav  blues.00059.wav  blues.00084.wav\r\n",
      "blues.00010.wav  blues.00035.wav  blues.00060.wav  blues.00085.wav\r\n",
      "blues.00011.wav  blues.00036.wav  blues.00061.wav  blues.00086.wav\r\n",
      "blues.00012.wav  blues.00037.wav  blues.00062.wav  blues.00087.wav\r\n",
      "blues.00013.wav  blues.00038.wav  blues.00063.wav  blues.00088.wav\r\n",
      "blues.00014.wav  blues.00039.wav  blues.00064.wav  blues.00089.wav\r\n",
      "blues.00015.wav  blues.00040.wav  blues.00065.wav  blues.00090.wav\r\n",
      "blues.00016.wav  blues.00041.wav  blues.00066.wav  blues.00091.wav\r\n",
      "blues.00017.wav  blues.00042.wav  blues.00067.wav  blues.00092.wav\r\n",
      "blues.00018.wav  blues.00043.wav  blues.00068.wav  blues.00093.wav\r\n",
      "blues.00019.wav  blues.00044.wav  blues.00069.wav  blues.00094.wav\r\n",
      "blues.00020.wav  blues.00045.wav  blues.00070.wav  blues.00095.wav\r\n",
      "blues.00021.wav  blues.00046.wav  blues.00071.wav  blues.00096.wav\r\n",
      "blues.00022.wav  blues.00047.wav  blues.00072.wav  blues.00097.wav\r\n",
      "blues.00023.wav  blues.00048.wav  blues.00073.wav  blues.00098.wav\r\n",
      "blues.00024.wav  blues.00049.wav  blues.00074.wav  blues.00099.wav\r\n"
     ]
    }
   ],
   "source": [
    "!ls music_dataset/genres/blues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aadce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/audio/main/generated/torchaudio.datasets.GTZAN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0be517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ecb704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4188c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANDataset(data.Dataset):\n",
    "    def __init__(self, data_path, split, num_samples, num_chunks):\n",
    "        self.data_path =  data_path if data_path else ''\n",
    "        self.split = split\n",
    "        self.num_samples = num_samples\n",
    "        self.num_chunks = num_chunks\n",
    "        self.genres = GTZAN_GENRES\n",
    "        self._get_song_list()\n",
    "\n",
    "\n",
    "    def _get_song_list(self):\n",
    "        list_filename = os.path.join(self.data_path, '%s_filtered.txt' % self.split)\n",
    "        with open(list_filename) as f:\n",
    "            lines = f.readlines()\n",
    "        self.song_list = [line.strip() for line in lines]\n",
    "\n",
    "\n",
    "    def _adjust_audio_length(self, wav):\n",
    "        if self.split == 'train':\n",
    "            random_index = random.randint(0, len(wav) - self.num_samples - 1)\n",
    "            wav = wav[random_index : random_index + self.num_samples]\n",
    "        else:\n",
    "            hop = (len(wav) - self.num_samples) // self.num_chunks\n",
    "            wav = np.array([wav[i * hop : i * hop + self.num_samples] for i in range(self.num_chunks)])\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.song_list[index]\n",
    "\n",
    "        # get genre\n",
    "        genre_name = line.split('/')[0]\n",
    "        genre_index = self.genres.index(genre_name)\n",
    "\n",
    "        # get audio\n",
    "        audio_filename = os.path.join(self.data_path, 'genres', line)\n",
    "        wav, fs = sf.read(audio_filename)\n",
    "\n",
    "        # adjust audio length\n",
    "        wav = self._adjust_audio_length(wav).astype('float32')\n",
    "\n",
    "\n",
    "        return wav, genre_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_list)\n",
    "\n",
    "def get_dataloader(data_path=None, \n",
    "                   split='train', \n",
    "                   num_samples=22050 * 29, \n",
    "                   num_chunks=1, \n",
    "                   batch_size=16, \n",
    "                   num_workers=0):\n",
    "    batch_size = batch_size if (split == 'train') else (batch_size // num_chunks)\n",
    "    data_loader = data.DataLoader(dataset=GTZANDataset(data_path, \n",
    "                                                       split, \n",
    "                                                       num_samples, \n",
    "                                                       num_chunks),\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=False,\n",
    "                                  num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57629bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: torch.Size([16, 639450])\n",
      "tensor([2, 7, 9, 6, 7, 5, 1, 0, 6, 9, 0, 3, 8, 7, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_dataloader(split='train', data_path=\"music_dataset\")\n",
    "iter_train_loader = iter(train_loader)\n",
    "train_wav, train_genre = next(iter_train_loader)\n",
    "\n",
    "print('training data shape: %s' % str(train_wav.shape))\n",
    "print(train_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adf32f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Convo(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, shape=3, pooling=2, dropout=0.1):\n",
    "        super(Convo, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, shape, padding=shape//2)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(pooling)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, wav):\n",
    "        out = self.conv(wav)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27df1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "\n",
    "class MusicClassifier(nn.Module):\n",
    "    def __init__(self, num_channels=16, \n",
    "                       sample_rate=22050, \n",
    "                       n_fft=1024, \n",
    "                       f_min=0.0, \n",
    "                       f_max=11025.0, \n",
    "                       num_mels=128, \n",
    "                       num_classes=10):\n",
    "        super(MusicClassifier, self).__init__()\n",
    "\n",
    "        # mel spectrogram\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, \n",
    "                                                            n_fft=n_fft, \n",
    "                                                            f_min=f_min, \n",
    "                                                            f_max=f_max, \n",
    "                                                            n_mels=num_mels)\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        self.input_bn = nn.BatchNorm2d(1)\n",
    "\n",
    "        # convolutional layers\n",
    "        self.layer1 = Convo(1, num_channels, pooling=(2, 3))\n",
    "        self.layer2 = Convo(num_channels, num_channels, pooling=(3, 4))\n",
    "        self.layer3 = Convo(num_channels, num_channels * 2, pooling=(2, 5))\n",
    "        self.layer4 = Convo(num_channels * 2, num_channels * 2, pooling=(3, 3))\n",
    "        self.layer5 = Convo(num_channels * 2, num_channels * 4, pooling=(3, 4))\n",
    "\n",
    "        # dense layers\n",
    "        self.dense1 = nn.Linear(num_channels * 4, num_channels * 4)\n",
    "        self.dense_bn = nn.BatchNorm1d(num_channels * 4)\n",
    "        self.dense2 = nn.Linear(num_channels * 4, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, wav):\n",
    "        # input Preprocessing\n",
    "        out = self.melspec(wav)\n",
    "        print(wav.shape, \"input shape\")\n",
    "        print(out.shape, \"mel shape\")\n",
    "        out = self.amplitude_to_db(out)\n",
    "        print(out.shape, \"after db shape\")\n",
    "\n",
    "        # input batch normalization\n",
    "        out = out.unsqueeze(1)\n",
    "        print(out.shape, \"a bit unsqueeze\")\n",
    "        out = self.input_bn(out)\n",
    "        print(out.shape, \"shape after batch normalization\")\n",
    "\n",
    "        # convolutional layers\n",
    "        out = self.layer1(out)\n",
    "        print(out.shape, \"shape after first convo\")\n",
    "        out = self.layer2(out)\n",
    "        print(out.shape, \"shape after second convo\")\n",
    "        out = self.layer3(out)\n",
    "        print(out.shape, \"shape after third convo\")\n",
    "        out = self.layer4(out)\n",
    "        print(out.shape, \"shape after fourth convo\")\n",
    "        out = self.layer5(out)\n",
    "        print(out.shape, \"shape after fifth convo\")\n",
    "        \n",
    "        # reshape. (batch_size, num_channels, 1, 1) -> (batch_size, num_channels)\n",
    "        out = out.reshape(len(out), -1)\n",
    "        print(out.shape, \"shape reshape\")\n",
    "\n",
    "        # dense layers\n",
    "        out = self.dense1(out)\n",
    "        print(out.shape, \"shape after first dense\")\n",
    "        out = self.dense_bn(out)\n",
    "        print(out.shape, \"shape after batch norm\")\n",
    "        out = self.relu(out)\n",
    "        print(out.shape, \"shape after relu\")\n",
    "        out = self.dropout(out)\n",
    "        print(out.shape, \"shape after dropout\")\n",
    "        out = self.dense2(out)\n",
    "        print(out.shape, \"shape after final dense\")\n",
    "        \n",
    "        print(\"================batch finished\")\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf569e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 639450]) input shape\n",
      "torch.Size([16, 128, 1249]) mel shape\n",
      "torch.Size([16, 128, 1249]) after db shape\n",
      "torch.Size([16, 1, 128, 1249]) a bit unsqueeze\n",
      "torch.Size([16, 1, 128, 1249]) shape after batch normalization\n",
      "torch.Size([16, 16, 64, 416]) shape after first convo\n",
      "torch.Size([16, 16, 21, 104]) shape after second convo\n",
      "torch.Size([16, 32, 10, 20]) shape after third convo\n",
      "torch.Size([16, 32, 3, 6]) shape after fourth convo\n",
      "torch.Size([16, 64, 1, 1]) shape after fifth convo\n",
      "torch.Size([16, 64]) shape reshape\n",
      "torch.Size([16, 64]) shape after first dense\n",
      "torch.Size([16, 64]) shape after batch norm\n",
      "torch.Size([16, 64]) shape after relu\n",
      "torch.Size([16, 64]) shape after dropout\n",
      "torch.Size([16, 10]) shape after final dense\n",
      "================batch finished\n",
      "torch.Size([16, 639450]) input shape\n",
      "torch.Size([16, 128, 1249]) mel shape\n",
      "torch.Size([16, 128, 1249]) after db shape\n",
      "torch.Size([16, 1, 128, 1249]) a bit unsqueeze\n",
      "torch.Size([16, 1, 128, 1249]) shape after batch normalization\n",
      "torch.Size([16, 16, 64, 416]) shape after first convo\n",
      "torch.Size([16, 16, 21, 104]) shape after second convo\n",
      "torch.Size([16, 32, 10, 20]) shape after third convo\n",
      "torch.Size([16, 32, 3, 6]) shape after fourth convo\n",
      "torch.Size([16, 64, 1, 1]) shape after fifth convo\n",
      "torch.Size([16, 64]) shape reshape\n",
      "torch.Size([16, 64]) shape after first dense\n",
      "torch.Size([16, 64]) shape after batch norm\n",
      "torch.Size([16, 64]) shape after relu\n",
      "torch.Size([16, 64]) shape after dropout\n",
      "torch.Size([16, 10]) shape after final dense\n",
      "================batch finished\n",
      "torch.Size([16, 639450]) input shape\n",
      "torch.Size([16, 128, 1249]) mel shape\n",
      "torch.Size([16, 128, 1249]) after db shape\n",
      "torch.Size([16, 1, 128, 1249]) a bit unsqueeze\n",
      "torch.Size([16, 1, 128, 1249]) shape after batch normalization\n",
      "torch.Size([16, 16, 64, 416]) shape after first convo\n",
      "torch.Size([16, 16, 21, 104]) shape after second convo\n",
      "torch.Size([16, 32, 10, 20]) shape after third convo\n",
      "torch.Size([16, 32, 3, 6]) shape after fourth convo\n",
      "torch.Size([16, 64, 1, 1]) shape after fifth convo\n",
      "torch.Size([16, 64]) shape reshape\n",
      "torch.Size([16, 64]) shape after first dense\n",
      "torch.Size([16, 64]) shape after batch norm\n",
      "torch.Size([16, 64]) shape after relu\n",
      "torch.Size([16, 64]) shape after dropout\n",
      "torch.Size([16, 10]) shape after final dense\n",
      "================batch finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19414/2530725454.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "music_classifier = MusicClassifier()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(music_classifier.parameters(), lr=0.001)\n",
    "valid_losses = []\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    # Train\n",
    "    music_classifier.train()\n",
    "    for (wav, genre_index) in train_loader:\n",
    "        wav = wav.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        # Forward\n",
    "        out = music_classifier(wav)\n",
    "        loss = loss_function(out, genre_index)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59dc4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
